{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdc769a2",
   "metadata": {},
   "source": [
    "## 1. Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be66d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc91d9",
   "metadata": {},
   "source": [
    "This notebook should serve as a guide to the creation of your Carnatic Music Instrument dataset. We will start with the loading of the dataset using the mirdata API, extract the relevant sections and instruments, apply any relevant processing steps, and store the dataset in an intuitive and accessible format.\n",
    "\n",
    "Typical Carnatic Music ensembles contain a wide-range of instruments. For this task we are going to focus on:\n",
    "\n",
    "- Voice\n",
    "- Violin\n",
    "- Mridangam\n",
    "\n",
    "You can refer to the instrumentation section of the [compIAM tutorial](https://mtg.github.io/IAM-tutorial-ismir22/indian_art_music/carnatic-music.html) for more information.\n",
    "\n",
    "The final dataset will be a collection of short audios corresponding to each of these instruments. They will be organised such that each can be retrieved according to the instrument they contain, the performer, the raga and a unique identifier (for reproducibility later).\n",
    "\n",
    "It is up to you to fill in each subsection with the relevant code to perform that task. If possible, try and split the sections amongst the project group to work in parallel. When the task is complete, you should try and abstract the code into .py files so that it can be ran without a python notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ee5df1",
   "metadata": {},
   "source": [
    "### Explore Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2399dd0",
   "metadata": {},
   "source": [
    "You can access the Saraga Carnatic dataset using the [mirdata API](https://github.com/mir-dataset-loaders/mirdata). You should already have the dataset downloaded on your machine in the mirdata repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6020868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mirdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3811c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGE FOR YOUR PATH TO DATASET\n",
    "data_home = r'C:\\UPF\\2023\\3rd Term\\Taller\\DataAPI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a539b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "saraga = mirdata.initialize('saraga_carnatic',data_home=data_home)\n",
    "#saraga.download() #Download the dataset only one time\n",
    "#saraga.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6099dc97",
   "metadata": {},
   "source": [
    "You can choose a random track using `.choice_track()`. This returns a Track object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5663de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_track = saraga.choice_track()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-waters",
   "metadata": {},
   "source": [
    "You can load all tracks and information to a dict using `.load_tracks()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-champion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_tracks = saraga.load_tracks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-musical",
   "metadata": {},
   "source": [
    "This returns a dict of `unique track identifier` : `track` object for each track."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-penguin",
   "metadata": {},
   "source": [
    "Track objects contain all filepaths of audios and metadata associated with the chosen track, and some information related to the recording itself (such as artist names and instruments). Remember, that for many recordings, we have 4 audio files relevant to our task...\n",
    "\n",
    "\n",
    "The path of the final mixed performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_track.audio_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-nickel",
   "metadata": {},
   "source": [
    "The path of the vocal microphone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-drinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_track.audio_vocal_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-memory",
   "metadata": {},
   "source": [
    "The path of the violin microphone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_track.audio_violin_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-wagon",
   "metadata": {},
   "source": [
    "And two mridangam microphones (one for each head):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_track.audio_mridangam_left_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-oliver",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_track.audio_mridangam_right_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-picnic",
   "metadata": {},
   "source": [
    "Navigate to these files and listen to the audios. What do you notice about them? Are they the same intensity? Is there any undesirable artifacts such as leaking or noise?\n",
    "\n",
    "Take note, the `mirdata` `Track` object will not have a `audio_vocal_path` (or vocal or mridangam) attribute if for the given track there is no multi-microphone recordings. Can you use this information to determine how many tracks we have multi-microphone recordings for? (HINT: You can check if an object has a specific attribute using the hasattr function: `hasattr(obj, \"<attribute_to_check_for>\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many tracks with multitrack recordings?\n",
    "counter = 0\n",
    "for i in saraga.track_ids:\n",
    "    if saraga.track(i).audio_violin_path is not None:\n",
    "        counter += 1\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-guess",
   "metadata": {},
   "source": [
    "Another important path is the metadata_path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = example_track.metadata\n",
    "print(metadata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-activation",
   "metadata": {},
   "source": [
    "Here you will find information relating to the recording such as artist names, instruments, raaga."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-university",
   "metadata": {},
   "source": [
    "Can you create some functions to explore these tracks and metadata? Perhaps it would be useful to know that JSON can be loaded in python using the `json` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open(\"DataAPI\\saraga1.5_carnatic\\Mahati at Arkay by Mahati\\Chinnanchiru Kiliye\\Chinnanchiru Kiliye.json\", 'r') as f:\n",
    "#     loaded_json = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(track_id):\n",
    "    \"\"\"\n",
    "    For <track_id>, return a dataframe of associated metadata\n",
    "    \"\"\"\n",
    "    metadata = saraga.track(track_id).metadata\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def get_performer(track_id):\n",
    "    \"\"\"\n",
    "    For <track_id>, return the performer\n",
    "    \"\"\"\n",
    "    performer = saraga.track(track_id).metadata[\"album_artists\"][0][\"name\"]\n",
    "    return performer\n",
    "\n",
    "def get_performance(track_id):\n",
    "    \"\"\"\n",
    "    For <track_id>, return the performance name\n",
    "    \"\"\"\n",
    "    performance = saraga.track(track_id).metadata[\"title\"]\n",
    "    return performance\n",
    "\n",
    "def get_raga(track_id):\n",
    "    \"\"\"\n",
    "    For <track_id>, return the raga name\n",
    "    \"\"\"\n",
    "    try:\n",
    "        raga = saraga.track(track_id).metadata[\"raaga\"][0][\"name\"]\n",
    "        return raga\n",
    "    except IndexError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_tonic(track_id):\n",
    "    \"\"\"\n",
    "    For <track_id>, return the tonic in hertz\n",
    "    \"\"\"\n",
    "    tonic = saraga.track(track_id).tonic\n",
    "    return tonic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-battlefield",
   "metadata": {},
   "source": [
    "How many ragas/performers/performances are available? How does that breakdown across performances for which we have multi-track recordings and those we dont?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "raagas=set()\n",
    "performers=set()\n",
    "performances=set()\n",
    "\n",
    "for i in saraga.load_tracks():\n",
    "    raagas.add(get_raga(i))\n",
    "    performers.add(get_performer(i))\n",
    "    performances.add(get_performance(i))\n",
    "raagas.remove(None)\n",
    "\n",
    "print(\"Raagas: \"+str(len(raagas)))\n",
    "print(\"Performers: \"+str(len(performers)))\n",
    "print(\"Performances: \"+str(len(performances)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9756db3",
   "metadata": {},
   "source": [
    "### Load Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-auction",
   "metadata": {},
   "source": [
    "The mirdata API returns paths to audio files associated with each track. Can you create some loaders to load an audio based on a given track name? \n",
    "\n",
    "**Hint**: The `librosa` library contains functions to load audio from file to an array of amplitude values. `y, sr = librosa.load(audio_path, sr=44100)`. `sr` in this instance refers to the sampling rate of the audio, i.e. how many individual amplitude energy values there are per second (typically 44100Hz). It is important to remember this resolution when converting between number of elements in the returned array and time in the track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa \n",
    "import numpy as np\n",
    "def load_mixed_audio(track_id):\n",
    "    \"\"\"\n",
    "    For <track_id>, return the loaded audio\n",
    "    \"\"\"\n",
    "    # code here\n",
    "    audio_path = saraga.track(track_id).audio_path\n",
    "    audio_array, sr = librosa.load(audio_path, sr=44100)    \n",
    "    return audio_array\n",
    "\n",
    "def load_violin_audio(track_id):\n",
    "    \"\"\"\n",
    "    For <track_id>, return the isolated violin track\n",
    "    \"\"\"\n",
    "    # code here\n",
    "    audio_path = saraga.track(track_id).audio_violin_path\n",
    "    audio_array, sr = librosa.load(audio_path, sr=44100)\n",
    "    return audio_array\n",
    "\n",
    "def load_voice_audio(track_id):\n",
    "    \"\"\"\n",
    "    For <track_id>, return the isolated voice track\n",
    "    \"\"\"\n",
    "    # code here\n",
    "    audio_path = saraga.track(track_id).audio_vocal_path\n",
    "    if audio_path != None:\n",
    "        audio_array, sr = librosa.load(audio_path, sr=44100)    \n",
    "        return audio_array\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_mridangam_audio(track_id):\n",
    "    \"\"\"\n",
    "    For <track_id>, return the isolated mridangam track\n",
    "    \"\"\"\n",
    "    # code here\n",
    "    audio_path_R = saraga.track(track_id).audio_mridangam_right_path\n",
    "    audio_path_L = saraga.track(track_id).audio_mridangam_left_path\n",
    "\n",
    "    audio_array_R, sr = librosa.load(audio_path_R, sr=44100,mono=False)    \n",
    "    audio_array_L, sr = librosa.load(audio_path_L, sr=44100,mono=False)\n",
    "    audio_array = librosa.to_mono(np.array([audio_array_L,audio_array_R]))\n",
    "    return audio_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-hours",
   "metadata": {},
   "source": [
    "### Listen to Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-beverage",
   "metadata": {},
   "source": [
    "Let's write some functions to listen and visualise these audio arrays in the notebook. \n",
    "\n",
    "**Hint**: You should find that the `Ipythoon.display.Audio` useful for playing audio inline in a Jupyter notebook.\n",
    "\n",
    "**Hint2**: Using the `matplotlib` library you can plot on two dimensions as so:\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x, y)\n",
    "```\n",
    "More information on enhancing these plots (e.g. with titles, axis labels and gridlines) can be found [here](https://matplotlib.org/stable/gallery/lines_bars_and_markers/simple_plot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "\n",
    "def plot_waveform(audio_array,amplitude=\"normal\"):\n",
    "    \"\"\"\n",
    "    Plot waveform for <audio_array> using matplotlib.pyplot\n",
    "    \"\"\"\n",
    "    \n",
    "    if amplitude==\"dB\":\n",
    "        audio_array = 20*np.log10(audio_array)\n",
    "    plt.figure().set_figwidth(20)\n",
    "    minutes = np.arange(len(audio_array))/(44100*60)\n",
    "    # plt.magnitude_spectrum(audio_array, Fs=44100, scale='dB', color='C1')\n",
    "    plt.plot(minutes,audio_array)\n",
    "    plt.xlabel('Minutes')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def play_audio(audio_array):\n",
    "    \"\"\"\n",
    "    Generate audio player for <audio_array> using Ipython library\n",
    "    \"\"\"\n",
    "    display(Audio(audio_array, rate=44100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-cornell",
   "metadata": {},
   "source": [
    "Are there any important observations about the mixed or isolated instrument tracks? What is the quality like, do you here all of the instruments clearly? Are there any differences between the audios of the individual instrument tracks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd0952",
   "metadata": {},
   "outputs": [],
   "source": [
    "song = \"113_Prathi_Vaaram_Vaaram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4937feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# play_audio(load_mixed_audio(song))\n",
    "plot_waveform(load_mixed_audio(song))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59535fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# play_audio(load_voice_audio(song))\n",
    "plot_waveform(load_voice_audio(song))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39678081",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_waveform(load_mridangam_audio(song))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_waveform(load_violin_audio(song))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-currency",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-lease",
   "metadata": {},
   "source": [
    "Are the isolated vocal tracks sufficiently isolated? Libraries like [`spleeter`](https://github.com/deezer/spleeter) can help separate singing sources from background instruments. Does it help here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spleeter.separator import Separator\n",
    "\n",
    "# def separate_voice(audio_path, isolated_audio_output_path):\n",
    "#     \"\"\"\n",
    "#     Apply spleeter source separation to input audio\n",
    "#     \"\"\"\n",
    "#     separator = Separator('spleeter:2stems')\n",
    "#     separator.separate_to_file(audio_path,isolated_audio_output_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-forum",
   "metadata": {},
   "source": [
    "How does the quality compare? Does spleeter work effectively? Do we lose any important information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dab131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate_voice(saraga.track(song).audio_path, \"spleeter_output.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62495c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocals_spleeter = librosa.load(r\"C:\\UPF\\2023\\3rd Term\\Taller\\Carnatic-Instrument-Classification\\Dataset Creation\\spleeter_output.mp3\\Prathi Vaaram Vaaram.mp3\\vocals.wav\", sr=44100)[0]\n",
    "# plot_waveform(vocals_spleeter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb45dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_waveform(load_voice_audio(song))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-november",
   "metadata": {},
   "source": [
    "### Tagging Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-genius",
   "metadata": {},
   "source": [
    "We want to tag our audios with whether or not a particular instrument is sounding. We can do this by identifying non-silent regions in the isolated tracks and tagging the mixed tracks with the instrument. The `librosa` library contains functionality for identifying silent regions in audio (`librosa.effects.split`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audios for each instrument track using previously defined functions\n",
    "# voice = load_voice_audio(song)\n",
    "# violin = load_violin_audio(song)\n",
    "# mridangam = load_mridangam_audio(song)\n",
    "# plot_waveform(voice)\n",
    "# play_audio(voice)\n",
    "# silent = librosa.effects.split(voice[:], top_db=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotNoSilence(silent,voice):\n",
    "    plt.figure().set_figwidth(10)\n",
    "    samples = np.arange(len(voice))\n",
    "    plt.plot(samples,voice)\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Amplitude')\n",
    "    for i in silent:\n",
    "        plt.annotate('', xy=(i[0], 0), xytext=(i[1], 0), xycoords='data', textcoords='data',\n",
    "                arrowprops={'arrowstyle': '|-|'})\n",
    "    plt.show()\n",
    "# plotNoSilence(silent,voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-university",
   "metadata": {},
   "source": [
    "Define a function to identify silent regions in an audio array. Look at the documentation for `librosa.effects.split` ([here](https://librosa.org/doc/main/generated/librosa.effects.split.html)). \n",
    "\n",
    "**Hint** - The `top_db` parameter tunes the harshness of the cut (a higher value considers louder regions as \"silent\"). Experiment with this value and compare the results with the audio plots. Do they correspond to what you visualise/hear?\n",
    "\n",
    "**Remember** - `librosa.effects.split` returns NON-silent intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_silence(audio_array,th=20):\n",
    "    \"\"\"\n",
    "    Return array of 0 and 1 (is silent/is not silent) for input <audio_array>. Returned array should\n",
    "    be equal in length to input array\n",
    "    \"\"\"\n",
    "    silent = librosa.effects.split(audio_array, top_db=th)\n",
    "    size = len(audio_array)\n",
    "    is_silent = np.zeros(size)\n",
    "    for i in silent:\n",
    "        is_silent[i[0]:i[1]] = 1\n",
    "    return is_silent\n",
    "def plotSilence(is_silent,audio_array):\n",
    "    plt.figure().set_figwidth(20)\n",
    "    samples =  [(x/44100)/60 for x in range(len(audio_array))]\n",
    "    plt.plot(samples,audio_array,zorder=0)\n",
    "    plt.xlabel('Time (minutes)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    silent_samples = ((np.where(is_silent==0)[0])/44100)/60\n",
    "    plt.scatter(silent_samples,np.zeros(len(silent_samples)),color='red',zorder=1,s=0.1)\n",
    "    plt.show()\n",
    "#\n",
    "def silenceAudio(audio_array,is_silent):\n",
    "    audio_array = audio_array * is_silent\n",
    "    return audio_array\n",
    "def removeOtherInstruments(silence,silence_to_remove):\n",
    "    for i,x in enumerate(silence_to_remove):\n",
    "        if x != 0:\n",
    "            silence[i]=0\n",
    "    return silence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-parish",
   "metadata": {},
   "source": [
    "Do these regions correspond to what you hear when playing the audio with `play_audio` or what you see with `plot_waveform`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mridangam = load_voice_audio(song)[:10516390]\n",
    "\n",
    "silent = detect_silence(mridangam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a1da00",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSilence(silent,mridangam)\n",
    "# play_audio(violin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-venue",
   "metadata": {},
   "source": [
    "### Extracting Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-court",
   "metadata": {},
   "source": [
    "We should now have all the tools necessary to load and annotated audio. We now want to extract small snippets of audio  from the mixed tracks across the dataset and annotate each of these snippets as either containing voice, mridangam, violin or none of the above (a single audio should be able to have more than one tag). \n",
    "\n",
    "It is important that we have examples for all combinations of tags (violin, voice, mridangam, none). Each sample should be of the same length (what should that length be? think about the two extreme cases of very very short and very long, what problems would arise in each of these cases).\n",
    "\n",
    "Each sample should have a unique identifier (index). The information relating to their tags should be stored in a metadata DataFrame where you can also find information about the performance.\n",
    "\n",
    "These should all be saved in individual audio files.\n",
    "\n",
    "Let us try with just on track to begin with..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-design",
   "metadata": {},
   "source": [
    "1. For a certain track id, load all audio files (mix, violin, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_array = load_mixed_audio(song)[:10516390]\n",
    "vocal_array = load_voice_audio(song)[:10516390]\n",
    "violin_array = load_violin_audio(song)[:10516390]\n",
    "vocal_array = load_voice_audio(song)[:10516390]\n",
    "mridangam = load_mridangam_audio(song)[:10516390]\n",
    "play_audio(mix_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-radius",
   "metadata": {},
   "source": [
    "2. Create a silent/non-silent array using `detect_silence()` defined earlier. \n",
    "\n",
    "      **Remember**: The mridangam has two tracks corresponding to it, you must combine them to identify whether either is sounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "violin_silence = detect_silence(violin_array,13)\n",
    "vocal_silence = detect_silence(vocal_array)\n",
    "mridangam_silnce = detect_silence(mridangam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b49482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# violin_silence = removeOtherInstruments(violin_silence,vocal_silence)\n",
    "plotSilence(violin_silence,violin_array)\n",
    "silencedViolin = silenceAudio(violin_array,violin_silence)\n",
    "play_audio(silencedViolin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a30cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSilence(vocal_silence,vocal_array)\n",
    "silencedVocal = silenceAudio(vocal_array,vocal_silence)\n",
    "play_audio(silencedVocal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166f146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSilence(mridangam_silnce,mridangam)\n",
    "silencedMridangam = silenceAudio(mridangam,mridangam_silnce)\n",
    "play_audio(silencedMridangam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-basin",
   "metadata": {},
   "source": [
    "3. Split mixed audio into small chunks using [numpy array indexing](https://numpy.org/doc/stable/user/basics.indexing.html) (the size of these chunks should be informed by the literature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_samples = 0.1*44100\n",
    "\n",
    "num_windows = int(len(mix_array)/w_samples)\n",
    "\n",
    "windows = np.array_split(mix_array, num_windows)  # Split the array into chunks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-horizon",
   "metadata": {},
   "source": [
    "4. Determine from your silent/non-silent arrays in Step 2 whether the chunk contains each instrument (voice, vocal, mridangam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "wviolin = np.array_split(violin_silence, num_windows)  # Split the array into chunks\n",
    "wmridangam = np.array_split(mridangam_silnce, num_windows)  # Split the array into chunks\n",
    "wvocal = np.array_split(vocal_silence, num_windows)  # Split the array into chunks\n",
    "data = {\n",
    "    \"lenth_window\":w_samples,\n",
    "    \"num_windows\":num_windows,\n",
    "    \"is_violin\": np.zeros(num_windows),\n",
    "    \"is_voice\":np.zeros(num_windows),\n",
    "    \"is_mrindangam\":np.zeros(num_windows)\n",
    "}\n",
    "for i,w in enumerate(windows):\n",
    "    if np.any(wviolin[i] == 1):\n",
    "        data[\"is_violin\"][i] = 1\n",
    "    if np.any(wmridangam[i]==1):\n",
    "        data[\"is_mrindangam\"][i] = 1\n",
    "    if np.any(wvocal[i]==1):\n",
    "        data[\"is_voice\"][i] = 1\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-congress",
   "metadata": {},
   "source": [
    "5. Save each audio with a unique index.\n",
    "\n",
    "    **Hint**: Audio arrays can be saved to file using the `soundfile` library:\n",
    "    `sf.write('<filename>.wav', <audio_array>, <sampling rate>)`\n",
    "    \n",
    "    **Remember**: Each audio chunk  needs to be assigned a unique index so as to be managed correctly later on. Feel free to use numbers, hashes or uuids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store audio with soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-rebate",
   "metadata": {},
   "source": [
    "6. Add row to metadata table containing relevant track information, index, and instrument annotations.\n",
    "\n",
    "    **Hint** - A `pandas` dataframe is a suitable place to store information relating to track and instrument annotations. You can create one using: \n",
    "\n",
    "    `import pandas as pd`\n",
    "\n",
    "    `df = pd.DataFrame(columns=<list of columns names>])`\n",
    "    \n",
    "    Add new rows using append:\n",
    "    \n",
    "    `df.append({dict of {column_name:value>, ignore_index=True)`\n",
    "    \n",
    "    And save using:\n",
    "    \n",
    "    `df.to_csv('<path.csv>', index=False)`\n",
    "    \n",
    "    **Remember** - This table should include the metadata relating to the track, the unique chunk index and a column indicating whether or not it includes each instrument \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=[\"window_index\",\"is_violin\",\"is_voice\",\"is_mrindangam\"])\n",
    "for i in range(num_windows):\n",
    "    df = df.append({\"window_index\":i,\"is_violin\":data[\"is_violin\"][i],\"is_voice\":data[\"is_voice\"][i],\"is_mrindangam\":data[\"is_mrindangam\"][i]},ignore_index=True)\n",
    "df.set_index(\"window_index\",inplace=True)\n",
    "df.to_csv(\"data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-diana",
   "metadata": {},
   "source": [
    "7. Repeat for many tracks and many chunks. Now you have written the individual code to do this for one track/chunk. Let's combine this and apply to a large number of tracks/chunks. Storing each with a unique index and a row in the metadata dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-validation",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-ocean",
   "metadata": {},
   "source": [
    "With our dataset created and saved in an intuitive and accessible format. Let's create some loaders to load the files and get metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dbad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample(index):\n",
    "    \"\"\"\n",
    "    Load sample with index, <index>\n",
    "    \"\"\"\n",
    "    return sample\n",
    "\n",
    "def get_metadata(index):\n",
    "    \"\"\"\n",
    "    Get metadata for sample with index, <index>\n",
    "    \"\"\"\n",
    "    sample = df.loc[index]\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51838d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metadata(39)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-concert",
   "metadata": {},
   "source": [
    "Typically, when datasets are presented, they are accompanied by some stats detailing their size and constiuent parts. What stats can you tell us about our dataset? Think about: number of seconds, performers, performances, instruments, ragas, filesizes etc.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-match",
   "metadata": {},
   "source": [
    "### Reproducible Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-chicago",
   "metadata": {},
   "source": [
    "Jupyter notebooks are great for experimenting, especially when visualisation or audio playback is required. However they are not great for reproducibility or source control. Can you abstract the code created here to .py file(s) so that the code can be ran in future without having to load the HTML notebook?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
